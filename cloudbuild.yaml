# Cloud build configuration to build a docker training image, submit it to the image registry, and run a training job
# https://cloud.google.com/build/docs/building/build-containers
# https://cloud.google.com/build/docs/configuring-builds/substitute-variable-values 
# https://cloud.google.com/build/docs/build-config-file-schema

substitutions:
  _IMAGE_NAME: "github.com/bieniekalexander/ml-model-promotion"
  _TAG_NAME: "${COMMIT_SHA}"
  _TRAINING_JOB_NAME: "${REPO_NAME}-${COMMIT_SHA}-build"
  _IMAGE_URI: "gcr.io/${PROJECT_ID}/${_IMAGE_NAME}:${_TAG_NAME}"
  _REGION: us-east1
  _MODEL_GCS_PATH: "gs://mw-ds-model-promotion-poc-res/census_model"
  _MODEL_NAME: "census_model_${_TAG_NAME}"
  _ENDPOINT_NAME: "census_model_endpoint"
timeout: 1800s

steps:
- id: "Build the training docker image"
  name: "gcr.io/cloud-builders/docker"
  args: ["build", "-t", "${_IMAGE_URI}", "."]

- id: "Submit the training image to the image registry"
  name: "gcr.io/cloud-builders/docker"
  args: ["push", "${_IMAGE_URI}"]

- id: "Create the CustomJobSpec YAML file for the Vertex AI Training Job"
  name: gcr.io/cloud-builders/curl
  entrypoint: /bin/bash
  args:
    - -c 
    - |
      # TODO maybe move args out of custom-jobs create command and move into here
      cat << EOF > /workspace/custom_job_spec.yml
      baseOutputDirectory:
        outputUriPrefix: "${_MODEL_GCS_PATH}"
      EOF

- id: "Write a python util to parse gcloud data"
  name: "gcr.io/cloud-builders/curl"
  entrypoint: /bin/bash
  args:
    - -c
    - |
      # this python util is helping me make up for the lack of jq on gcloud build images:
      # jq '.[] | select(.displayName=="${_MODEL_NAME}") | .name'
      # | sed 's/"//g' \
      # | awk -F'/' '{  print $6 }'
      cat <<EOF > /workspace/get_id_by_display_name.py
      from __future__ import print_function
      import argparse
      import json

      def get_id_by_display_name(lst, display_name):
        """ Find the json object with the specified display name, and return the id field at the end of the object's name """
        items = list(filter(lambda x: x['displayName']==display_name, lst))

        if items:
          item = items[0]
          return item['name'].rsplit('/', 1)[-1]
        else:
          return ""


      if __name__ == "__main__":
        parser = argparse.ArgumentParser(description='Get id of google api object from display name')
        parser.add_argument('--display-name', dest='display_name', required=True,
                          help='name of google object to find')
        parser.add_argument('--json-path', dest='json_path', required=True,
                          help='path to the json file to read')

        args = parser.parse_args()
        obj = json.load(open(args.json_path, 'r'))

        assert isinstance(obj, list)

        print(get_id_by_display_name(obj, args.display_name), end='')
      EOF

 
# https://cloud.google.com/sdk/gcloud/reference/ai/custom-jobs/create
# https://cloud.google.com/vertex-ai/docs/reference/rest/v1/CustomJobSpec
# TODO when configured, this will create the serving container but not create the VAI model from it
- id: "Run a training job using the docker image we just uploaded, and wait for the job to finish"
  name: "gcr.io/cloud-builders/gcloud"
  entrypoint: /bin/bash
  args:
    - -c 
    - |
      gcloud ai custom-jobs create \
        --worker-pool-spec=container-image-uri=${_IMAGE_URI},machine-type=n2-standard-4 \
        --config=/workspace/custom_job_spec.yml \
        --display-name=${_TRAINING_JOB_NAME} \
        --region=${_REGION} \
        2>&1 \
        | grep describe \
        | sed 's|.*/||' \
        > /workspace/custom_job_id

      echo "Waiting for job $(</workspace/custom_job_id)"

      until gcloud ai custom-jobs describe projects/${PROJECT_NUMBER}/locations/${_REGION}/customJobs/$(</workspace/custom_job_id) | grep -e "^endTime:"
      do
        sleep 30
      done
      
      echo "done waiting for job $(</workspace/custom_job_id)"
      JOB_STATE=$(gcloud ai custom-jobs describe projects/${PROJECT_NUMBER}/locations/${_REGION}/customJobs/$(</workspace/custom_job_id) | grep -e "^state:" | awk '{print $2}')

      if [ $$JOB_STATE != "JOB_STATE_SUCCEEDED" ] ; then
        echo "Training job $(</workspace/custom_job_id) failed with JOB_STATE=$$JOB_STATE"
        exit 1
      else
        echo "Training job $(</workspace/custom_job_id) succeeded with JOB_STATE=$$JOB_STATE"
      fi

# https://cloud.google.com/sdk/gcloud/reference/ai/models
# TODO a model should be able to receive new versions, but it looks like vertex AI doesn't allow this through the CLI yet
# TODO how are we selecting the right container to build from? It's hardcoded right now
- id: "Upload/Update a Vertex AI Model using the serialized model we just uploaded"
  name: gcr.io/cloud-builders/gcloud
  entrypoint: /bin/bash
  args:
    - -c 
    - |
      # check if the model currently exists, as identified by display name
      # TODO what if display name isn't unique? Is there a better identifier?
      gcloud ai models list --region=${_REGION} --format=json > /workspace/gcloud_response.json
      python /workspace/get_id_by_display_name.py --display-name=${_MODEL_NAME} --json-path=/workspace/gcloud_response.json > /workspace/model_id
      rm /workspace/gcloud_response.json

      if [ -s /workspace/model_id ]; then
        echo "Model Uploaded model $(</workspace/model_id) exists, TODO update model"
        # gcloud ai models update ... # TODO update when model update command exists
        exit 2
        echo "Updated model $(</workspace/model_id)"
      else
        echo "Model does not exist, uploading new model"
        gcloud ai models upload \
        --container-image-uri=us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-5:latest \
        --artifact-uri=$_MODEL_GCS_PATH \
        --display-name=${_MODEL_NAME} \
        --region=$_REGION

        gcloud ai models list --region=${_REGION} --format=json > /workspace/gcloud_response.json
        python /workspace/get_id_by_display_name.py --display-name=${_MODEL_NAME} --json-path=/workspace/gcloud_response.json > /workspace/model_id
        rm /workspace/gcloud_response.json

        echo "Uploaded model $(</workspace/model_id)"
      fi


# https://cloud.google.com/sdk/gcloud/reference/ai/endpoints
# TODO how are we selecting the right container to build from? It's hardcoded right now
- id: "Create/Update a Vertex AI Endpoint and deploy the new model to it"
  name: gcr.io/cloud-builders/gcloud
  entrypoint: /bin/bash
  args:
    - -c 
    - |
      # check if the endpoint currently exists, as identified by display name
      # TODO what if endpoint name isn't unique? Is there a better identifier?
      gcloud ai endpoints list --region=${_REGION} --format=json > /workspace/gcloud_response.json
      python /workspace/get_id_by_display_name.py --display-name=${_ENDPOINT_NAME} --json-path=/workspace/gcloud_response.json > /workspace/endpoint_id
      rm /workspace/gcloud_response.json

      if [ -s /workspace/endpoint_id ]; then
        echo "Endpoint $(</workspace/endpoint_id) exists, skipping endpoint creation"
      else
        echo "Endpoint does not exist yet, creating endpoint"
        gcloud ai endpoints create \
          --display-name=${_ENDPOINT_NAME} \
          --region=${_REGION}

        gcloud ai endpoints list --region=${_REGION} --format=json > /workspace/gcloud_response.json
        python /workspace/get_id_by_display_name.py --display-name=${_ENDPOINT_NAME} --json-path=/workspace/gcloud_response.json > /workspace/endpoint_id
        rm /workspace/gcloud_response.json
        echo "Created endpoint $(</workspace/endpoint_id)"
      fi

      gcloud ai endpoints deploy-model $(</workspace/endpoint_id) \
        --project=${PROJECT_ID} \
        --region=${_REGION} \
        --display-name=${_MODEL_NAME} \
        --model=$(</workspace/model_id)
      
      echo "Deployed model $(</workspace/model_id) to endpoint $(</workspace/endpoint_id)"